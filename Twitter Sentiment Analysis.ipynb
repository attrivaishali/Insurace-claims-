{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adc9ab1",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "In this project, we try to implement an NLP Twitter sentiment analysis model that helps to overcome the challenges of sentiment classification of tweets. We will be classifying the tweets into positive or negative sentiments. The necessary details regarding the dataset involving the Twitter sentiment analysis project are:\n",
    "tweet_id\n",
    "tweet\n",
    "sentiment¶\n",
    "\n",
    "Twitter Sentiment Analysis: Project Pipeline\n",
    "\n",
    "The various steps involved in the Machine Learning Pipeline are:\n",
    "\n",
    "-Import Necessary Dependencies\n",
    "\n",
    "-Read and Load the Dataset\n",
    "\n",
    "-Exploratory Data Analysis\n",
    "\n",
    "-Data Visualization of Target Variables\n",
    "\n",
    "-Data Preprocessing\n",
    "\n",
    "-Splitting our data into Train and Test sets.\n",
    "\n",
    "-Transforming Dataset using TF-IDF Vectorizer\n",
    "\n",
    "-Function for Model Evaluation\n",
    "\n",
    "-Model Building\n",
    "\n",
    "-Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af6088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as myplot\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf5b0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1\n"
     ]
    }
   ],
   "source": [
    " import matplotlib\n",
    "\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8ccdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR conda.notices.fetch:get_channel_notice_response(68): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/main/notices.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))> for channel: defaults url: https://repo.anaconda.com/pkgs/main/notices.json\n",
      "ERROR conda.notices.fetch:get_channel_notice_response(68): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/msys2/notices.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))> for channel: defaults url: https://repo.anaconda.com/pkgs/msys2/notices.json\n",
      "ERROR conda.notices.fetch:get_channel_notice_response(68): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/r/notices.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))> for channel: defaults url: https://repo.anaconda.com/pkgs/r/notices.json\n",
      "ERROR conda.notices.fetch:get_channel_notice_response(68): Request error <HTTPSConnectionPool(host='conda.anaconda.org', port=443): Max retries exceeded with url: /conda-forge/notices.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))> for channel: conda-forge url: https://conda.anaconda.org/conda-forge/notices.json\n",
      "\n",
      "CondaSSLError: Encountered an SSL error. Most likely a certificate verification issue.\n",
      "\n",
      "Exception: HTTPSConnectionPool(host='conda.anaconda.org', port=443): Max retries exceeded with url: /conda-forge/win-64/current_repodata.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... failed\n"
     ]
    }
   ],
   "source": [
    " conda install -c conda-forge matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c184bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "DATASET_COLUMNS=['tweet_id', 'tweet', 'sentiment']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "\n",
    "df= pd.read_csv(\"Twitter Sentiment Analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc95e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "785eb01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'tweet', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "805a1d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data is 7274\n"
     ]
    }
   ],
   "source": [
    "print('length of data is', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5a5b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7274, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45db627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7274 entries, 0 to 7273\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   7274 non-null   int64 \n",
      " 1   tweet      7273 non-null   object\n",
      " 2   sentiment  7274 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dfb9e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id      int64\n",
       "tweet        object\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d1a545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull().any(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f6bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of columns in the data is:   3\n",
      "Count of rows in the data is:   7274\n"
     ]
    }
   ],
   "source": [
    "print('Count of columns in the data is:  ', len(df.columns))\n",
    "print('Count of rows in the data is:  ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ec7efc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1701, 1851, 2689, ..., 5378, 2173, 3162], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd251d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7274"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution for dataset.\n",
    "ax = df.groupby('tweet_id').count().plot(kind='tweet_id', title='Distribution of data',legend=False)\n",
    "matplotlib.pyplot.xtick(['Negative','Positive'])\n",
    "# Storing data in lists.\n",
    "tweet, sentiment = list(df['tweet']), list(df['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fced24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='tweet_id', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e949c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution for dataset.\n",
    "ax = df.groupby('tweet_id').count().plot(kind='bar', title='Distribution of data',legend=False)\n",
    "ax.set_xticklabels(['Negative','Positive'])\n",
    "# Storing data in lists.\n",
    "text, sentiment = list(df['tweet']), list(df['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[['tweet','tweet_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_id'] = data['tweet_id'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['tweet_id'] == 1]\n",
    "data_neg = data[data['tweet_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(20000)]\n",
    "data_neg = data_neg.iloc[:int(20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5030bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet']=dataset['tweet'].str.lower()\n",
    "dataset['tweet'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8329c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2567c58",
   "metadata": {},
   "source": [
    "##### Cleaning and removing the above stop words list from the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb35251",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: cleaning_stopwords(text))\n",
    "dataset['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeed76a",
   "metadata": {},
   "source": [
    "##### Cleaning and removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b05951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "dataset['tweet']= dataset['tweet'].apply(lambda x: cleaning_punctuations(x))\n",
    "dataset['tweet'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbab6da",
   "metadata": {},
   "source": [
    "##### Cleaning and removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
    "dataset['tweet'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f880a79",
   "metadata": {},
   "source": [
    "##### Cleaning and removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d573153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda x: cleaning_URLs(x))\n",
    "dataset['tweet'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1acb3c",
   "metadata": {},
   "source": [
    "##### Cleaning and removing numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4033325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['tweet'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fa13e",
   "metadata": {},
   "source": [
    "##### Getting tokenization of tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'w+')\n",
    "dataset['tweet'] = dataset['tweet'].apply(tokenizer.tokenize)\n",
    "dataset['tweet'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edc10c",
   "metadata": {},
   "source": [
    "##### Applying stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4398d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['tweet']= dataset['tweet'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656398d",
   "metadata": {},
   "source": [
    "#####  Applying lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3329efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a3ff9",
   "metadata": {},
   "source": [
    "##### Separating input feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.tweet\n",
    "y=data.tweet_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbb21b",
   "metadata": {},
   "source": [
    "#####  Plot a cloud of words for negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = data['tweet'][:800000]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9a3d3",
   "metadata": {},
   "source": [
    "#####  Plot a cloud of words for positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20256ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data['tweet'][800000:]\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "              collocations=False).generate(\" \".join(data_pos))\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b79da0",
   "metadata": {},
   "source": [
    "##### Splitting Our Data Into Train and Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the 95% data for training data and 5% for testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =26105111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1e1af",
   "metadata": {},
   "source": [
    "#### Transforming the Dataset Using TF-IDF Vectorizer\n",
    "\n",
    "#### Fit the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c35bd",
   "metadata": {},
   "source": [
    "##### Transform the data using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36921106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "# Predict values for Test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "# Print the evaluation metrics for the dataset.\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Compute and plot the Confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "categories = ['Negative','Positive']\n",
    "group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "xticklabels = categories, yticklabels = categories)\n",
    "plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n",
    "plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNBmodel = BernoulliNB()\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)\n",
    "y_pred1 = BNBmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC CURVE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVCmodel)\n",
    "y_pred2 = SVCmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d191875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC CURVE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b097c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)\n",
    "y_pred3 = LRmodel.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc45e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred3)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC CURVE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591d64d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We hope through this article, you got a basic of how Sentimental Analysis is used to understand public emotions behind people’s tweets. As you’ve read in this article, Twitter Sentimental Analysis helps us preprocess the data (tweets) using different methods and feed it into ML models to give the best accuracy.\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "Twitter Sentimental Analysis is used to identify as well as classify the sentiments that are expressed in the text source.\n",
    "Logistic Regression, SVM, and Naive Bayes are some of the ML algorithms that can be used for Twitter Sentimental Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef5157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
